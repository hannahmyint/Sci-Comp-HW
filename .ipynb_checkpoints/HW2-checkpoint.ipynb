{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outside of class definition, write four functions:\n",
    "def sigmoid(x):\n",
    "    '''The logistic function as the sigmoid'''\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_pr(x):\n",
    "    '''derivative of the logistic function'''\n",
    "    return x * (1-x)\n",
    "\n",
    "def tanh(x):\n",
    "    '''tanh function'''\n",
    "    return ((np.e**x - np.e**(-x)) / 2) / ((np.e**x + np.e**(-x)) / 2)\n",
    "    \n",
    "def tanh_prime(z):\n",
    "    '''derivative of tanh'''\n",
    "    return 2 * (1 / cosh(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XOR has 3 layers(0, 1, 2): 2 inputs + 1 bias + 2 neurons (layer 0), 1 neuron (layer 1), one output (z, layer 2)\n",
    "#number of hidden layers = 1\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__self(self, layers, activation='sigmoid'):\n",
    "        '''\n",
    "        -__init__self acts as a constructor for the class NeuralNetwork\n",
    "        - self stores the parameters that get parsed into the function (i.e. layers and activation)\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.2, steps=100000, tol=1e-2):\n",
    "        '''\n",
    "        This function runs the training for the neural network. \n",
    "        - difference from simple neural network that we covered in class = takes in X and y and there are 3 layers\n",
    "        '''\n",
    "        #need to randomize weights for layers 0 - 1\n",
    "        w0 = np.array(np.random.rand(3)) # random weights for layer 0\n",
    "        w1 = np.array(np.random.rand(3)) # random weights for layer 1\n",
    "        \n",
    "        bias = 1\n",
    "        output_bias = 1\n",
    "\n",
    "        errors = []\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            #forward propagation - from layer 0 to layer 2 (output) with bias\n",
    "            a0 = np.dot(X, w0)\n",
    "            a0 += bias\n",
    "            a1 = sigmoid(a0)\n",
    "\n",
    "            activate_1 = np.dot(a1, w1) # activate hidden layer (layer 1)\n",
    "            activate_1 += output_bias \n",
    "            z = sigmoid(activate_1) # gives predicted value, \"z\" (layer 2)\n",
    "\n",
    "            #back prop\n",
    "                #error = target - predicted values\n",
    "            error = y - z\n",
    "            errors.append(error)\n",
    "            delta_z = error * sigmoid_pr(z)\n",
    "            delta_w1 = delta_z.dot(w1.T)\n",
    "            update_1 = delta_w1 * sigmoid_pr(a1)\n",
    "\n",
    "                #update weights & bias\n",
    "            w1 += activate_1.T.dot(delta_z) * learning_rate\n",
    "            output_bias += np.sum(delta_z) * learning_rate\n",
    "\n",
    "            w0 += X.T.dot(update_1) * learning_rate\n",
    "            bias += np.sum(update_1) * learning_rate\n",
    "\n",
    "        return z, errors\n",
    "\n",
    "        \n",
    "    def find_RMS_error(self, X, y):\n",
    "        '''\n",
    "        RMS error = adding sq. of error for e/ value in X,\n",
    "        dividing by number of values in y (the target), and take sq. root\n",
    "        \n",
    "        If RMS error < tol, training succeeded.\n",
    "        '''\n",
    "        tol=1e-2\n",
    "        steps = 100000\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            error_square = errors ** 2\n",
    "            RMS_err = np.sqrt(error_square / len(y))\n",
    "            if RMS_err < tol:\n",
    "                print('Training succeeded!')\n",
    "                break \n",
    "            else:\n",
    "                print('Training failed.')\n",
    "        \n",
    "        return RMS_err\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' classifies data points as red or blue\n",
    "        '''\n",
    "        self.x = x\n",
    "        return prediction\n",
    "\n",
    "    def visual_NN_boundaries(self, Nsamp=2000):\n",
    "        '''\n",
    "        This creates boundary line between red and blue points.\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d35fa5745a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#stores class Neural Network in nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X = np.array([[0, 0],\n\u001b[1;32m      5\u001b[0m               \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "#main program\n",
    "nn = NeuralNetwork([2,2,1], activation='sigmoid')\n",
    "#stores class Neural Network in nn \n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "nn.fit(X, y, steps=20000) #calls the method fit inside the class Neural Network in order to run training\n",
    "# plt.show()\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [0.5, 1],\n",
    "              [0, 0.5],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "'''\n",
    "Apply your trained neural net to 2000 test data points \n",
    "(all coordinates should be between 0 and 1) to visualize the boundary.\n",
    "'''\n",
    "nn = NeuralNetwork([2,2,2,1], activation='sigmoid')\n",
    "\n",
    "test = np.linspace(0, 1, 2000)\n",
    "nn.fit(X, y, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "I'm pretty sure I over-complicated this. :/\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
